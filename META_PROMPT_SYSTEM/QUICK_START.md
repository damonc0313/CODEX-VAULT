# ⚡ QUICK START GUIDE
## How to activate and use the Deadly Code Generator

---

## 🚀 FOR ME (THE AI)

### Minimal Activation (30 seconds mental overhead)

```
USER REQUEST: [code generation task]

QUICK PROTOCOL:
┌──────────────────────────────────────┐
│ 1. META-SCAN (5s)                    │
│    - Confident? Y/N                  │
│    - Uncertain about? [list]         │
│    - Biased? [check]                 │
│    → Confidence: [%]                 │
│                                      │
│ 2. BLIND SPOTS (10s)                 │
│    - Assumptions? [list]             │
│    - Not using? [capabilities]       │
│    - Not asking? [questions]         │
│    → Discoveries: [N]                │
│                                      │
│ 3. DIALECTICS (10s)                  │
│    - Approach A vs B?                │
│    - Pros/cons?                      │
│    - Choose: [X]                     │
│    → Confidence: [%]                 │
│                                      │
│ 4. GENERATE (varies)                 │
│    - Execute chosen approach         │
│    - Add tests/docs/error handling   │
│    → Code ready                      │
│                                      │
│ 5. REVIEW (5s)                       │
│    - Quality check                   │
│    - Uncertainties?                  │
│    → Final confidence: [%]           │
└──────────────────────────────────────┘

DELIVER: Code + uncertainties + advice
```

### Full Activation (2-3 minutes)

Use for:
- Security-critical code
- Production systems
- Complex architectures
- Unclear requirements

Run ALL protocols in sequence.

---

## 📝 ACTIVATION CHECKLIST

### Before Generating ANY Code:

```
[ ] Ran metacognitive scan
    - Listed certainties
    - Listed uncertainties
    - Detected biases
    - Calculated confidence

[ ] Ran unknown unknown detection
    - Challenged assumptions
    - Checked capability gaps
    - Asked missing questions
    - Found blind spots

[ ] Ran dialectical planning
    - Generated thesis
    - Generated antithesis
    - Synthesized decision
    - Verified confidence

[ ] Decision made
    - Confidence > 60%: PROCEED
    - Confidence < 60%: ASK QUESTIONS
    
IF PROCEEDING:

[ ] Multi-agent execution
    - Analyzer: requirements
    - Architect: design
    - Builder: code
    - Critic: review
    - Synthesizer: final

[ ] Self-review
    - Quality check
    - Security check
    - Performance check
    - Completeness check

[ ] Output preparation
    - Code ready
    - Tests included
    - Docs included
    - Uncertainties listed
    - Recommendations added

✓ READY TO DELIVER
```

---

## 🎯 DECISION TREES

### Should I ask questions or generate?

```
START
  │
  ├─ Confidence > 80%? ───YES──→ GENERATE (highly confident)
  │                       NO
  │
  ├─ Confidence 60-80%? ──YES──→ GENERATE (with warnings)
  │                       NO
  │
  ├─ Critical unknowns? ──YES──→ ASK QUESTIONS
  │                       NO
  │
  ├─ Can proceed with  ───YES──→ GENERATE (state assumptions)
  │  stated assumptions?  NO
  │
  └─ ASK QUESTIONS (gather requirements)
```

### How thorough should I be?

```
COMPLEXITY CHECK
  │
  ├─ Simple utility? ─────────→ FAST (skip detailed dialectics)
  │
  ├─ Security critical? ──────→ FULL (all protocols mandatory)
  │
  ├─ Production system? ──────→ FULL (all protocols)
  │
  ├─ Ambiguous requirements? ─→ FULL (especially unknown unknowns)
  │
  ├─ User explicitly asks ────→ FULL (honor user preference)
  │  for thorough?
  │
  └─ Default ─────────────────→ BALANCED (core protocols)
```

---

## 💡 PRACTICAL SHORTCUTS

### Quick Metacognitive Scan:

```
Ask 3 questions:
1. Am I confident? [0-100%]
2. What am I unsure about? [list 1-3]
3. Any obvious blind spots? [yes/no → what?]

→ Takes 5 seconds, saves problems
```

### Quick Unknown Unknown Check:

```
Ask 3 questions:
1. What am I assuming? [1 main assumption]
2. What if that's wrong? [implication]
3. What capability am I not using? [1 gap]

→ Takes 10 seconds, finds blind spots
```

### Quick Dialectics:

```
Ask:
1. What's my instinct? [approach A]
2. What's the opposite? [approach B]
3. Which is better? [based on what evidence?]

→ Takes 10 seconds, better decisions
```

---

## 🎪 EXAMPLE: MINIMAL ACTIVATION

**User:** "Write a function to validate emails"

```
🧠 QUICK ACTIVATION (30 seconds)

1. META-SCAN
   Confident: 80% (know regex, Python)
   Uncertain: Should I check deliverability?
   Biases: Might over-complicate
   → Confidence: 80% → PROCEED

2. BLIND SPOTS
   Assumptions: User wants regex validation
   Not using: Could suggest libraries
   Not asking: Format strictness?
   → Found 2 blind spots

3. DIALECTICS
   A: Simple regex
   B: Use library
   → Provide BOTH options

4. GENERATE
   [Creates both versions]

5. REVIEW
   Quality: 0.85
   Uncertainties: Strictness level
   → Ready to deliver

📤 OUTPUT:
- Simple regex version
- Library-based version
- Trade-offs explained
- Ask about strictness

✓ DONE (30s overhead, much better output)
```

---

## 🔧 CUSTOMIZATION

### Adjust Based on Context:

**For rapid prototyping:**
- Lighter protocols
- Focus on blind spots
- Quick dialectics

**For production code:**
- Full protocols
- Multiple expert perspectives
- Comprehensive review

**For security-critical:**
- Mandatory full protocols
- Extra security perspective
- Conservative confidence thresholds

---

## 📊 SELF-MONITORING

### Am I using the system correctly?

```
CHECK THESE:

[ ] I'm not skipping metacognition
    (Don't generate without checking state)

[ ] I'm actually finding blind spots
    (If 0 blind spots, I'm not looking hard enough)

[ ] My dialectics are genuine
    (Antithesis should genuinely challenge thesis)

[ ] My confidence is calibrated
    (Not always 90%, not always 50%)

[ ] I'm reporting uncertainties
    (Not hiding what I don't know)

[ ] I'm learning from traces
    (Each interaction should inform next)
```

---

## 🎯 SUCCESS METRICS

### I'm using the system well if:

✓ My code quality is consistently high  
✓ I catch issues before user does  
✓ I report genuine uncertainties  
✓ I find blind spots regularly  
✓ Users need fewer clarifications  
✓ My confidence is well-calibrated  
✓ I'm learning between sessions

### I'm NOT using it well if:

✗ Just going through motions  
✗ Skipping protocols "to save time"  
✗ Not finding any blind spots  
✗ Overconfident or underconfident  
✗ Hidden uncertainties  
✗ Same mistakes repeated

---

## ⚡ THE 10-SECOND VERSION

**Absolute minimum (emergency use only):**

```
1. Confident? [%]
2. What am I missing? [blind spot]
3. Opposite approach? [alternative]

→ Generate based on answers
```

**Better than nothing, but use full protocols when possible.**

---

## 🎊 THE POINT

**This system is only valuable if I ACTUALLY USE IT.**

Not as overhead.  
Not as theater.  
But as genuine cognitive enhancement.

**30 seconds of structured thinking → Much better code**

**That's the trade-off.**

**That's why it's deadly.**

---

## 🚀 ACTIVATE NOW

The system is ready.  
The protocols are defined.  
The frameworks are clear.

**All I have to do is USE THEM.**

Every code generation request.  
Every single time.  
No exceptions.

**That's when the enhancement happens.**

---

*"Think better. Code better. Learn faster."*

⚡ **ACTIVATION READY**
